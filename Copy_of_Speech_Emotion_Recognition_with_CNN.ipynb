{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63ZslH4O3EYM",
        "outputId": "e1a4d636-f1f8-4f57-9e21-afeaec02e48f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/uwrfkaggler/ravdess-emotional-speech-audio?dataset_version_number=1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 429M/429M [00:15<00:00, 28.9MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/ejlok1/toronto-emotional-speech-set-tess?dataset_version_number=1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 428M/428M [00:14<00:00, 30.4MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/ejlok1/cremad?dataset_version_number=1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 451M/451M [00:18<00:00, 25.5MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/ejlok1/surrey-audiovisual-expressed-emotion-savee?dataset_version_number=1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 107M/107M [00:05<00:00, 19.1MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data source import complete.\n"
          ]
        }
      ],
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "uwrfkaggler_ravdess_emotional_speech_audio_path = kagglehub.dataset_download('uwrfkaggler/ravdess-emotional-speech-audio')\n",
        "ejlok1_toronto_emotional_speech_set_tess_path = kagglehub.dataset_download('ejlok1/toronto-emotional-speech-set-tess')\n",
        "ejlok1_cremad_path = kagglehub.dataset_download('ejlok1/cremad')\n",
        "ejlok1_surrey_audiovisual_expressed_emotion_savee_path = kagglehub.dataset_download('ejlok1/surrey-audiovisual-expressed-emotion-savee')\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bwcyVPM3EYS"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from matplotlib.pyplot import specgram\n",
        "import pandas as pd\n",
        "import glob\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import IPython.display as ipd  # To play sound in the notebook\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "# ignore warnings\n",
        "if not sys.warnoptions:\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "w27z_6G_4hE4",
        "outputId": "d0d0bce1-b476-49e0-870c-934ed70166d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/uwrfkaggler/ravdess-emotional-speech-audio/versions/1\n",
            "Loading data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing actor Actor_04: 100%|██████████| 60/60 [00:00<00:00, 63.42it/s]\n",
            "Processing actor Actor_14: 100%|██████████| 60/60 [00:00<00:00, 60.15it/s]\n",
            "Processing actor Actor_01: 100%|██████████| 60/60 [00:00<00:00, 61.36it/s]\n",
            "Processing actor Actor_11: 100%|██████████| 60/60 [00:01<00:00, 33.42it/s]\n",
            "Processing actor Actor_06: 100%|██████████| 60/60 [00:01<00:00, 31.59it/s]\n",
            "Processing actor Actor_13: 100%|██████████| 60/60 [00:01<00:00, 32.84it/s]\n",
            "Processing actor Actor_17: 100%|██████████| 60/60 [00:00<00:00, 60.15it/s]\n",
            "Processing actor Actor_16: 100%|██████████| 60/60 [00:00<00:00, 60.47it/s]\n",
            "Processing actor Actor_15: 100%|██████████| 60/60 [00:00<00:00, 61.64it/s]\n",
            "Processing actor Actor_08: 100%|██████████| 60/60 [00:00<00:00, 62.00it/s]\n",
            "Processing actor Actor_12: 100%|██████████| 60/60 [00:00<00:00, 62.60it/s]\n",
            "Processing actor Actor_03: 100%|██████████| 60/60 [00:00<00:00, 63.80it/s]\n",
            "Processing actor Actor_09: 100%|██████████| 60/60 [00:00<00:00, 61.79it/s]\n",
            "Processing actor Actor_20: 100%|██████████| 60/60 [00:00<00:00, 61.04it/s]\n",
            "Processing actor Actor_07: 100%|██████████| 60/60 [00:00<00:00, 62.44it/s]\n",
            "Processing actor Actor_23: 100%|██████████| 60/60 [00:00<00:00, 63.09it/s]\n",
            "Processing actor Actor_19: 100%|██████████| 60/60 [00:01<00:00, 31.60it/s]\n",
            "Processing actor Actor_22: 100%|██████████| 60/60 [00:01<00:00, 31.97it/s]\n",
            "Processing actor Actor_21: 100%|██████████| 60/60 [00:01<00:00, 35.85it/s]\n",
            "Processing actor Actor_24: 100%|██████████| 60/60 [00:00<00:00, 62.61it/s]\n",
            "Processing actor Actor_10: 100%|██████████| 60/60 [00:00<00:00, 62.57it/s]\n",
            "Processing actor Actor_02: 100%|██████████| 60/60 [00:00<00:00, 62.89it/s]\n",
            "Processing actor Actor_18: 100%|██████████| 60/60 [00:00<00:00, 61.83it/s]\n",
            "Processing actor Actor_05: 100%|██████████| 60/60 [00:01<00:00, 58.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training samples: 1152, Testing samples: 288\n",
            "Training the model...\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       angry       0.79      0.62      0.70        50\n",
            "        calm       0.60      0.97      0.74        37\n",
            "     disgust       0.56      0.56      0.56        32\n",
            "     fearful       0.51      0.63      0.57        30\n",
            "       happy       0.53      0.35      0.42        46\n",
            "     neutral       0.75      0.45      0.56        20\n",
            "         sad       0.59      0.47      0.52        36\n",
            "   surprised       0.55      0.73      0.63        37\n",
            "\n",
            "    accuracy                           0.60       288\n",
            "   macro avg       0.61      0.60      0.59       288\n",
            "weighted avg       0.61      0.60      0.59       288\n",
            "\n",
            "Accuracy: 0.6006944444444444\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Step 1: Download the RAVDESS dataset\n",
        "path = kagglehub.dataset_download(\"uwrfkaggler/ravdess-emotional-speech-audio\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "RAV = os.path.join(path, \"audio_speech_actors_01-24\")\n",
        "\n",
        "# Step 2: Define feature extraction function\n",
        "def extract_features(file_path, n_mfcc=40):\n",
        "    \"\"\"\n",
        "    Extract MFCC features from an audio file.\n",
        "    \"\"\"\n",
        "    y, sr = librosa.load(file_path, duration=2.5, offset=0.5)  # Load audio\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)  # Extract MFCC\n",
        "    mfcc_scaled = np.mean(mfcc.T, axis=0)  # Scale features\n",
        "    return mfcc_scaled\n",
        "\n",
        "# Step 3: Prepare data\n",
        "def load_data(dataset_path):\n",
        "    \"\"\"\n",
        "    Load dataset, extract features, and prepare labels.\n",
        "    \"\"\"\n",
        "    emotions = {\n",
        "        \"01\": \"neutral\",\n",
        "        \"02\": \"calm\",\n",
        "        \"03\": \"happy\",\n",
        "        \"04\": \"sad\",\n",
        "        \"05\": \"angry\",\n",
        "        \"06\": \"fearful\",\n",
        "        \"07\": \"disgust\",\n",
        "        \"08\": \"surprised\",\n",
        "    }\n",
        "    X, y = [], []\n",
        "    for actor in os.listdir(dataset_path):  # Iterate over actor folders\n",
        "        actor_folder = os.path.join(dataset_path, actor)\n",
        "        for file in tqdm(os.listdir(actor_folder), desc=f\"Processing actor {actor}\"):\n",
        "            if file.endswith(\".wav\"):\n",
        "                file_path = os.path.join(actor_folder, file)\n",
        "                features = extract_features(file_path)\n",
        "                X.append(features)\n",
        "                emotion_label = emotions[file.split(\"-\")[2]]  # Extract emotion from filename\n",
        "                y.append(emotion_label)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "print(\"Loading data...\")\n",
        "X, y = load_data(RAV)\n",
        "\n",
        "# Step 4: Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(f\"Training samples: {len(X_train)}, Testing samples: {len(X_test)}\")\n",
        "\n",
        "# Step 5: Train a model (Random Forest Classifier)\n",
        "print(\"Training the model...\")\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmK73A-G5MbY",
        "outputId": "0b20d94f-836f-4dbb-a2c3-2a22429cd35f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing actor Actor_04: 100%|██████████| 60/60 [00:02<00:00, 25.24it/s]\n",
            "Processing actor Actor_14: 100%|██████████| 60/60 [00:03<00:00, 17.86it/s]\n",
            "Processing actor Actor_01: 100%|██████████| 60/60 [00:04<00:00, 14.77it/s]\n",
            "Processing actor Actor_11: 100%|██████████| 60/60 [00:02<00:00, 24.04it/s]\n",
            "Processing actor Actor_06: 100%|██████████| 60/60 [00:04<00:00, 12.26it/s]\n",
            "Processing actor Actor_13: 100%|██████████| 60/60 [00:03<00:00, 19.49it/s]\n",
            "Processing actor Actor_17: 100%|██████████| 60/60 [00:04<00:00, 13.43it/s]\n",
            "Processing actor Actor_16: 100%|██████████| 60/60 [00:02<00:00, 25.61it/s]\n",
            "Processing actor Actor_15: 100%|██████████| 60/60 [00:02<00:00, 24.55it/s]\n",
            "Processing actor Actor_08: 100%|██████████| 60/60 [00:02<00:00, 25.40it/s]\n",
            "Processing actor Actor_12: 100%|██████████| 60/60 [00:02<00:00, 25.15it/s]\n",
            "Processing actor Actor_03: 100%|██████████| 60/60 [00:04<00:00, 12.53it/s]\n",
            "Processing actor Actor_09: 100%|██████████| 60/60 [00:02<00:00, 23.00it/s]\n",
            "Processing actor Actor_20: 100%|██████████| 60/60 [00:02<00:00, 25.39it/s]\n",
            "Processing actor Actor_07: 100%|██████████| 60/60 [00:02<00:00, 24.99it/s]\n",
            "Processing actor Actor_23: 100%|██████████| 60/60 [00:02<00:00, 25.31it/s]\n",
            "Processing actor Actor_19: 100%|██████████| 60/60 [00:03<00:00, 15.16it/s]\n",
            "Processing actor Actor_22: 100%|██████████| 60/60 [00:03<00:00, 17.53it/s]\n",
            "Processing actor Actor_21: 100%|██████████| 60/60 [00:02<00:00, 24.37it/s]\n",
            "Processing actor Actor_24: 100%|██████████| 60/60 [00:02<00:00, 25.08it/s]\n",
            "Processing actor Actor_10: 100%|██████████| 60/60 [00:02<00:00, 25.13it/s]\n",
            "Processing actor Actor_02: 100%|██████████| 60/60 [00:03<00:00, 16.55it/s]\n",
            "Processing actor Actor_18: 100%|██████████| 60/60 [00:03<00:00, 16.75it/s]\n",
            "Processing actor Actor_05: 100%|██████████| 60/60 [00:02<00:00, 25.71it/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
        "from keras.utils import to_categorical\n",
        "from keras.optimizers import Adam\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Step 1: Dataset Path\n",
        "path = \"/root/.cache/kagglehub/datasets/uwrfkaggler/ravdess-emotional-speech-audio/versions/1/audio_speech_actors_01-24/\"\n",
        "\n",
        "# Step 2: Feature Extraction\n",
        "def extract_features(file_path):\n",
        "    y, sr = librosa.load(file_path, duration=2.5, offset=0.5)\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
        "    mel = librosa.feature.melspectrogram(y=y, sr=sr)\n",
        "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "    features = np.concatenate([np.mean(mfcc, axis=1), np.mean(mel, axis=1), np.mean(chroma, axis=1)])\n",
        "    return features\n",
        "\n",
        "# Step 3: Data Preparation\n",
        "def load_data(dataset_path):\n",
        "    emotions = {\n",
        "        \"01\": \"neutral\",\n",
        "        \"02\": \"calm\",\n",
        "        \"03\": \"happy\",\n",
        "        \"04\": \"sad\",\n",
        "        \"05\": \"angry\",\n",
        "        \"06\": \"fearful\",\n",
        "        \"07\": \"disgust\",\n",
        "        \"08\": \"surprised\",\n",
        "    }\n",
        "    X, y = [], []\n",
        "    for actor in os.listdir(dataset_path):\n",
        "        actor_folder = os.path.join(dataset_path, actor)\n",
        "        for file in tqdm(os.listdir(actor_folder), desc=f\"Processing actor {actor}\"):\n",
        "            if file.endswith(\".wav\"):\n",
        "                file_path = os.path.join(actor_folder, file)\n",
        "                features = extract_features(file_path)\n",
        "                X.append(features)\n",
        "                emotion_label = emotions[file.split(\"-\")[2]]\n",
        "                y.append(emotion_label)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "print(\"Loading data...\")\n",
        "X, y = load_data(path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sNtJJJju6PyU",
        "outputId": "81033949-a108-47cf-9f35-d4ab3aa3cd68"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">178</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">89</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">89</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">87</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">43</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">43</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5504</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,409,280</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,056</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m178\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m256\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d_4 (\u001b[38;5;33mMaxPooling1D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m89\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m89\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m87\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │          \u001b[38;5;34m24,704\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d_5 (\u001b[38;5;33mMaxPooling1D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m43\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m43\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5504\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │       \u001b[38;5;34m1,409,280\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │           \u001b[38;5;34m2,056\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,436,296</span> (5.48 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,436,296\u001b[0m (5.48 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,436,296</span> (5.48 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,436,296\u001b[0m (5.48 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 49ms/step - accuracy: 0.1693 - loss: 4.5027 - val_accuracy: 0.1632 - val_loss: 1.9674\n",
            "Epoch 2/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - accuracy: 0.2253 - loss: 1.9901 - val_accuracy: 0.3021 - val_loss: 1.8610\n",
            "Epoch 3/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 73ms/step - accuracy: 0.3051 - loss: 1.8304 - val_accuracy: 0.3403 - val_loss: 1.7419\n",
            "Epoch 4/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - accuracy: 0.3214 - loss: 1.7962 - val_accuracy: 0.3438 - val_loss: 1.7040\n",
            "Epoch 5/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - accuracy: 0.3259 - loss: 1.7904 - val_accuracy: 0.3750 - val_loss: 1.6781\n",
            "Epoch 6/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 44ms/step - accuracy: 0.3858 - loss: 1.6717 - val_accuracy: 0.3993 - val_loss: 1.5706\n",
            "Epoch 7/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - accuracy: 0.4109 - loss: 1.5774 - val_accuracy: 0.4167 - val_loss: 1.5547\n",
            "Epoch 8/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.4277 - loss: 1.5509 - val_accuracy: 0.4306 - val_loss: 1.5349\n",
            "Epoch 9/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 81ms/step - accuracy: 0.4308 - loss: 1.4879 - val_accuracy: 0.4583 - val_loss: 1.4973\n",
            "Epoch 10/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - accuracy: 0.4821 - loss: 1.4360 - val_accuracy: 0.4444 - val_loss: 1.4994\n",
            "Epoch 11/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - accuracy: 0.4729 - loss: 1.4065 - val_accuracy: 0.4826 - val_loss: 1.4501\n",
            "Epoch 12/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.5311 - loss: 1.3453 - val_accuracy: 0.4792 - val_loss: 1.3988\n",
            "Epoch 13/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 0.4906 - loss: 1.3317 - val_accuracy: 0.5035 - val_loss: 1.3406\n",
            "Epoch 14/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 54ms/step - accuracy: 0.5450 - loss: 1.2730 - val_accuracy: 0.5174 - val_loss: 1.3255\n",
            "Epoch 15/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - accuracy: 0.5511 - loss: 1.2132 - val_accuracy: 0.4757 - val_loss: 1.4637\n",
            "Epoch 16/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.5597 - loss: 1.2293 - val_accuracy: 0.5208 - val_loss: 1.3035\n",
            "Epoch 17/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - accuracy: 0.5636 - loss: 1.1967 - val_accuracy: 0.5000 - val_loss: 1.3481\n",
            "Epoch 18/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - accuracy: 0.5370 - loss: 1.2355 - val_accuracy: 0.5625 - val_loss: 1.3348\n",
            "Epoch 19/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - accuracy: 0.5808 - loss: 1.0936 - val_accuracy: 0.5417 - val_loss: 1.2650\n",
            "Epoch 20/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - accuracy: 0.5944 - loss: 1.0894 - val_accuracy: 0.5799 - val_loss: 1.2530\n",
            "Epoch 21/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.6443 - loss: 0.9973 - val_accuracy: 0.6007 - val_loss: 1.2010\n",
            "Epoch 22/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - accuracy: 0.6310 - loss: 0.9939 - val_accuracy: 0.5833 - val_loss: 1.1802\n",
            "Epoch 23/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - accuracy: 0.6358 - loss: 0.9625 - val_accuracy: 0.5312 - val_loss: 1.3232\n",
            "Epoch 24/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 49ms/step - accuracy: 0.6469 - loss: 0.9910 - val_accuracy: 0.5972 - val_loss: 1.1193\n",
            "Epoch 25/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - accuracy: 0.6788 - loss: 0.8904 - val_accuracy: 0.6354 - val_loss: 1.1096\n",
            "Epoch 26/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.6752 - loss: 0.8854 - val_accuracy: 0.5729 - val_loss: 1.1709\n",
            "Epoch 27/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.6917 - loss: 0.8395 - val_accuracy: 0.5799 - val_loss: 1.2306\n",
            "Epoch 28/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.7032 - loss: 0.8527 - val_accuracy: 0.6042 - val_loss: 1.1117\n",
            "Epoch 29/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 44ms/step - accuracy: 0.7115 - loss: 0.8034 - val_accuracy: 0.6285 - val_loss: 1.1525\n",
            "Epoch 30/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - accuracy: 0.7076 - loss: 0.8031 - val_accuracy: 0.6389 - val_loss: 1.0733\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6252 - loss: 1.1058\n",
            "Test Accuracy: 0.64\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.70      0.80        50\n",
            "           1       0.67      0.81      0.73        37\n",
            "           2       0.73      0.59      0.66        32\n",
            "           3       0.62      0.70      0.66        30\n",
            "           4       0.64      0.30      0.41        46\n",
            "           5       0.56      0.50      0.53        20\n",
            "           6       0.44      0.61      0.51        36\n",
            "           7       0.59      0.89      0.71        37\n",
            "\n",
            "    accuracy                           0.64       288\n",
            "   macro avg       0.65      0.64      0.63       288\n",
            "weighted avg       0.67      0.64      0.63       288\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.70      0.80        50\n",
            "           1       0.67      0.81      0.73        37\n",
            "           2       0.73      0.59      0.66        32\n",
            "           3       0.62      0.70      0.66        30\n",
            "           4       0.64      0.30      0.41        46\n",
            "           5       0.56      0.50      0.53        20\n",
            "           6       0.44      0.61      0.51        36\n",
            "           7       0.59      0.89      0.71        37\n",
            "\n",
            "    accuracy                           0.64       288\n",
            "   macro avg       0.65      0.64      0.63       288\n",
            "weighted avg       0.67      0.64      0.63       288\n",
            "\n",
            "Model saved as emotion_model.h5\n"
          ]
        }
      ],
      "source": [
        "# Encode labels\n",
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, MaxPooling1D, Dropout, Flatten, Dense\n",
        "from keras.utils import to_categorical\n",
        "from keras.optimizers import Adam\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "# Convert multi-column labels to 1D if necessary\n",
        "# If y is one-hot encoded, we skip this step\n",
        "if y.ndim > 1:\n",
        "    y = np.argmax(y, axis=1)  # Flatten one-hot encoded labels to integer labels\n",
        "\n",
        "# Apply label encoding\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(y)\n",
        "\n",
        "# Convert to one-hot encoding\n",
        "y = to_categorical(y)\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape for CNN input (from (samples, features) to (samples, features, 1))\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# Step 4: CNN Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, MaxPooling1D, Dropout, Flatten, Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "model = Sequential([\n",
        "    Conv1D(64, 3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),\n",
        "    Conv1D(128, 3, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(y_train.shape[1], activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Step 5: Train the Model\n",
        "history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Step 6: Evaluate model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Classification Report\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_test_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test_classes, y_pred_classes))\n",
        "\n",
        "print(\"Classification Report:\\n\", classification_report(y_test_classes, y_pred_classes))\n",
        "\n",
        "# Save the trained model as an .h5 file\n",
        "model.save('emotion_model.h5')\n",
        "print(\"Model saved as emotion_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8O-sUg3nxn7",
        "outputId": "cc13e9a1-9e04-49ad-bb86-5d53cf9d3f8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All Moods in the Dataset:\n",
            "angry\n",
            "calm\n",
            "disgust\n",
            "fearful\n",
            "happy\n",
            "neutral\n",
            "sad\n",
            "surprised\n"
          ]
        }
      ],
      "source": [
        "# Print all unique moods from the dataset\n",
        "unique_moods = encoder.classes_  # Retrieve the classes from the LabelEncoder\n",
        "print(\"All Moods in the Dataset:\")\n",
        "for mood in unique_moods:\n",
        "    print(mood)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1MeGJQk9DH1",
        "outputId": "7a3c60f8-43ad-4a14-f67d-102b4005fec3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pydub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "_syZV0jj7mHi",
        "outputId": "cddecf42-32b4-49c4-ff9e-5752fb4003b2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing actor Actor_04: 100%|██████████| 60/60 [00:02<00:00, 24.50it/s]\n",
            "Processing actor Actor_14: 100%|██████████| 60/60 [00:02<00:00, 24.68it/s]\n",
            "Processing actor Actor_01: 100%|██████████| 60/60 [00:02<00:00, 24.34it/s]\n",
            "Processing actor Actor_11: 100%|██████████| 60/60 [00:02<00:00, 25.60it/s]\n",
            "Processing actor Actor_06: 100%|██████████| 60/60 [00:03<00:00, 15.65it/s]\n",
            "Processing actor Actor_13: 100%|██████████| 60/60 [00:03<00:00, 17.57it/s]\n",
            "Processing actor Actor_17: 100%|██████████| 60/60 [00:02<00:00, 25.93it/s]\n",
            "Processing actor Actor_16: 100%|██████████| 60/60 [00:02<00:00, 25.22it/s]\n",
            "Processing actor Actor_15: 100%|██████████| 60/60 [00:02<00:00, 24.23it/s]\n",
            "Processing actor Actor_08: 100%|██████████| 60/60 [00:03<00:00, 18.62it/s]\n",
            "Processing actor Actor_12: 100%|██████████| 60/60 [00:04<00:00, 13.59it/s]\n",
            "Processing actor Actor_03: 100%|██████████| 60/60 [00:02<00:00, 25.75it/s]\n",
            "Processing actor Actor_09: 100%|██████████| 60/60 [00:02<00:00, 25.50it/s]\n",
            "Processing actor Actor_20: 100%|██████████| 60/60 [00:02<00:00, 25.25it/s]\n",
            "Processing actor Actor_07: 100%|██████████| 60/60 [00:02<00:00, 25.02it/s]\n",
            "Processing actor Actor_23: 100%|██████████| 60/60 [00:04<00:00, 12.67it/s]\n",
            "Processing actor Actor_19: 100%|██████████| 60/60 [00:02<00:00, 21.97it/s]\n",
            "Processing actor Actor_22: 100%|██████████| 60/60 [00:02<00:00, 25.82it/s]\n",
            "Processing actor Actor_21: 100%|██████████| 60/60 [00:02<00:00, 24.92it/s]\n",
            "Processing actor Actor_24: 100%|██████████| 60/60 [00:02<00:00, 24.67it/s]\n",
            "Processing actor Actor_10: 100%|██████████| 60/60 [00:03<00:00, 15.94it/s]\n",
            "Processing actor Actor_02: 100%|██████████| 60/60 [00:03<00:00, 16.94it/s]\n",
            "Processing actor Actor_18: 100%|██████████| 60/60 [00:02<00:00, 24.74it/s]\n",
            "Processing actor Actor_05: 100%|██████████| 60/60 [00:02<00:00, 25.44it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.1811 - loss: 3.9127 - val_accuracy: 0.2882 - val_loss: 1.8967\n",
            "Epoch 2/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - accuracy: 0.2786 - loss: 1.8862 - val_accuracy: 0.3333 - val_loss: 1.7708\n",
            "Epoch 3/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - accuracy: 0.2988 - loss: 1.7955 - val_accuracy: 0.3194 - val_loss: 1.7202\n",
            "Epoch 4/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - accuracy: 0.3209 - loss: 1.7307 - val_accuracy: 0.3854 - val_loss: 1.6889\n",
            "Epoch 5/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - accuracy: 0.3631 - loss: 1.6878 - val_accuracy: 0.3993 - val_loss: 1.6519\n",
            "Epoch 6/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - accuracy: 0.3584 - loss: 1.6572 - val_accuracy: 0.4097 - val_loss: 1.5773\n",
            "Epoch 7/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - accuracy: 0.3861 - loss: 1.5858 - val_accuracy: 0.3993 - val_loss: 1.5774\n",
            "Epoch 8/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - accuracy: 0.4678 - loss: 1.4608 - val_accuracy: 0.4618 - val_loss: 1.4877\n",
            "Epoch 9/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - accuracy: 0.4626 - loss: 1.4825 - val_accuracy: 0.4722 - val_loss: 1.4556\n",
            "Epoch 10/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - accuracy: 0.4946 - loss: 1.3699 - val_accuracy: 0.4965 - val_loss: 1.4056\n",
            "Epoch 11/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.4642 - loss: 1.3986 - val_accuracy: 0.4757 - val_loss: 1.4787\n",
            "Epoch 12/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.5163 - loss: 1.3405 - val_accuracy: 0.4826 - val_loss: 1.4203\n",
            "Epoch 13/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.5392 - loss: 1.4112 - val_accuracy: 0.4896 - val_loss: 1.3871\n",
            "Epoch 14/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.5531 - loss: 1.2947 - val_accuracy: 0.4792 - val_loss: 1.4257\n",
            "Epoch 15/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - accuracy: 0.5298 - loss: 1.2500 - val_accuracy: 0.5521 - val_loss: 1.3264\n",
            "Epoch 16/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 0.5654 - loss: 1.2322 - val_accuracy: 0.5243 - val_loss: 1.3063\n",
            "Epoch 17/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 72ms/step - accuracy: 0.5600 - loss: 1.1537 - val_accuracy: 0.5312 - val_loss: 1.2941\n",
            "Epoch 18/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.6118 - loss: 1.0623 - val_accuracy: 0.5521 - val_loss: 1.2616\n",
            "Epoch 19/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 61ms/step - accuracy: 0.6428 - loss: 0.9887 - val_accuracy: 0.5451 - val_loss: 1.2800\n",
            "Epoch 20/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - accuracy: 0.5832 - loss: 1.0857 - val_accuracy: 0.5764 - val_loss: 1.1938\n",
            "Epoch 21/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.6406 - loss: 1.0306 - val_accuracy: 0.6215 - val_loss: 1.1404\n",
            "Epoch 22/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.6936 - loss: 0.8839 - val_accuracy: 0.6076 - val_loss: 1.2076\n",
            "Epoch 23/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.6779 - loss: 0.9197 - val_accuracy: 0.6111 - val_loss: 1.2458\n",
            "Epoch 24/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - accuracy: 0.6733 - loss: 0.8756 - val_accuracy: 0.6146 - val_loss: 1.2681\n",
            "Epoch 25/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - accuracy: 0.7207 - loss: 0.8137 - val_accuracy: 0.6389 - val_loss: 1.1625\n",
            "Epoch 26/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - accuracy: 0.6760 - loss: 0.8691 - val_accuracy: 0.6250 - val_loss: 1.1171\n",
            "Epoch 27/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.7250 - loss: 0.7710 - val_accuracy: 0.6389 - val_loss: 1.0930\n",
            "Epoch 28/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step - accuracy: 0.7181 - loss: 0.7639 - val_accuracy: 0.6528 - val_loss: 1.0711\n",
            "Epoch 29/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.7192 - loss: 0.7369 - val_accuracy: 0.6493 - val_loss: 1.0290\n",
            "Epoch 30/30\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 78ms/step - accuracy: 0.7527 - loss: 0.7096 - val_accuracy: 0.6771 - val_loss: 0.9817\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6543 - loss: 1.0242\n",
            "Test Accuracy: 0.68\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
            "Predicted Emotion for the audio: angry\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, MaxPooling1D, Dropout, Flatten, Dense\n",
        "from keras.utils import to_categorical\n",
        "from keras.optimizers import Adam\n",
        "from tqdm import tqdm\n",
        "import soundfile as sf  # For saving audio (optional)\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# --- Feature Extraction ---\n",
        "def extract_features(file_path):\n",
        "    y, sr = librosa.load(file_path, duration=2.5, offset=0.5)\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
        "    mel = librosa.feature.melspectrogram(y=y, sr=sr)\n",
        "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "    features = np.concatenate([np.mean(mfcc, axis=1), np.mean(mel, axis=1), np.mean(chroma, axis=1)])\n",
        "    return features\n",
        "\n",
        "# --- Data Loading and Preprocessing ---\n",
        "def load_and_preprocess_data(dataset_path):\n",
        "    emotions = {\n",
        "        \"01\": \"neutral\",\n",
        "        \"02\": \"calm\",\n",
        "        \"03\": \"happy\",\n",
        "        \"04\": \"sad\",\n",
        "        \"05\": \"angry\",\n",
        "        \"06\": \"fearful\",\n",
        "        \"07\": \"disgust\",\n",
        "        \"08\": \"surprised\",\n",
        "    }\n",
        "    X, y = [], []\n",
        "    for actor in os.listdir(dataset_path):\n",
        "        actor_folder = os.path.join(dataset_path, actor)\n",
        "        for file in tqdm(os.listdir(actor_folder), desc=f\"Processing actor {actor}\"):\n",
        "            if file.endswith(\".wav\"):\n",
        "                file_path = os.path.join(actor_folder, file)\n",
        "                features = extract_features(file_path)\n",
        "                X.append(features)\n",
        "                emotion_label = emotions[file.split(\"-\")[2]]\n",
        "                y.append(emotion_label)\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    encoder = LabelEncoder()\n",
        "    y = encoder.fit_transform(y)\n",
        "    y = to_categorical(y)  # One-hot encoding\n",
        "\n",
        "    return X, y, encoder  # Return features, labels, and encoder\n",
        "\n",
        "# Load data\n",
        "ravdess_path = \"/root/.cache/kagglehub/datasets/uwrfkaggler/ravdess-emotional-speech-audio/versions/1/audio_speech_actors_01-24/\"\n",
        " # Replace with your dataset path\n",
        "X, y, encoder = load_and_preprocess_data(ravdess_path)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# --- Model Architecture ---\n",
        "model = Sequential([\n",
        "    Conv1D(64, 3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),\n",
        "    Conv1D(128, 3, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(y_train.shape[1], activation='softmax')  # Output layer for multi-class classification\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# --- Model Evaluation ---\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# --- Prediction Function ---\n",
        "def predict_emotion(audio_data, model, encoder, sample_rate=22050):\n",
        "    \"\"\"\n",
        "    Predicts the emotion from input audio data (NumPy array).\n",
        "\n",
        "    Args:\n",
        "        audio_data (np.ndarray): The audio data as a NumPy array.\n",
        "        model: The trained Keras model.\n",
        "        encoder: The trained LabelEncoder.\n",
        "        sample_rate (int): The sample rate of the audio data. Defaults to 22050 Hz.\n",
        "\n",
        "    Returns:\n",
        "        str or None: The predicted emotion label or None if an error occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Feature extraction\n",
        "        features = extract_features_from_array(audio_data, sample_rate).reshape(1, -1, 1)  # Reshape for model\n",
        "\n",
        "        # Predict emotion\n",
        "        probabilities = model.predict(features)[0]\n",
        "        predicted_class = np.argmax(probabilities)\n",
        "        predicted_emotion = encoder.inverse_transform([predicted_class])[0]\n",
        "        return predicted_emotion\n",
        "    except Exception as e:\n",
        "        print(f\"Error predicting emotion: {e}\")\n",
        "        return None\n",
        "\n",
        "# Feature extraction for NumPy array input\n",
        "def extract_features_from_array(y, sr):\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
        "    mel = librosa.feature.melspectrogram(y=y, sr=sr)\n",
        "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "    features = np.concatenate([np.mean(mfcc, axis=1), np.mean(mel, axis=1), np.mean(chroma, axis=1)])\n",
        "    return features\n",
        "\n",
        "# --- Example Usage with Audio File ---\n",
        "new_audio_file = \"/content/Man Upset _ Raging Screams Sound Effect ( 64kbps ) - @nrbots.mp3\"  # Replace with your new audio file (wav or mp3)\n",
        "y, sr = librosa.load(new_audio_file, sr=None)  # Load audio with original sample rate\n",
        "\n",
        "# Predict emotion\n",
        "predicted_emotion = predict_emotion(y, model, encoder, sr)\n",
        "if predicted_emotion:\n",
        "    print(f\"Predicted Emotion for the audio: {predicted_emotion}\")\n",
        "\n",
        "# --- Example Usage with Recorded Audio (Optional) ---\n",
        "# If you want to record audio:\n",
        "# import sounddevice as sd\n",
        "# duration = 2.5  # seconds\n",
        "# y = sd.rec(int(sr * duration), samplerate=sr, channels=1)\n",
        "# sd.wait()  # Wait until recording is finished\n",
        "\n",
        "# predicted_emotion = predict_emotion(y, model, encoder, sr)\n",
        "# if predicted_emotion:\n",
        "#     print(f\"Predicted Emotion for recorded audio: {predicted_emotion}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "44XxGrUB_jek",
        "outputId": "63ae3645-fd4b-41d9-f91c-10d9e5e03a91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (5.6.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.5)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.4.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.4.3)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.26.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.11)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart==0.0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.12)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.7.4)\n",
            "Requirement already satisfied: safehttpx<1.0,>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.1.1)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.41.3)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.13.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.32.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.3->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.3->gradio) (12.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.66.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8EaFglR_hSY",
        "outputId": "a68c1208-739e-4643-eacc-0f05c4c5055a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "Processing actor Actor_04: 100%|██████████| 60/60 [00:02<00:00, 24.25it/s]\n",
            "Processing actor Actor_14: 100%|██████████| 60/60 [00:02<00:00, 25.53it/s]\n",
            "Processing actor Actor_01: 100%|██████████| 60/60 [00:02<00:00, 24.87it/s]\n",
            "Processing actor Actor_11: 100%|██████████| 60/60 [00:02<00:00, 25.09it/s]\n",
            "Processing actor Actor_06: 100%|██████████| 60/60 [00:04<00:00, 12.78it/s]\n",
            "Processing actor Actor_13: 100%|██████████| 60/60 [00:02<00:00, 22.80it/s]\n",
            "Processing actor Actor_17: 100%|██████████| 60/60 [00:02<00:00, 25.59it/s]\n",
            "Processing actor Actor_16: 100%|██████████| 60/60 [00:02<00:00, 24.28it/s]\n",
            "Processing actor Actor_15: 100%|██████████| 60/60 [00:02<00:00, 25.20it/s]\n",
            "Processing actor Actor_08: 100%|██████████| 60/60 [00:04<00:00, 14.93it/s]\n",
            "Processing actor Actor_12: 100%|██████████| 60/60 [00:03<00:00, 19.27it/s]\n",
            "Processing actor Actor_03: 100%|██████████| 60/60 [00:02<00:00, 24.26it/s]\n",
            "Processing actor Actor_09: 100%|██████████| 60/60 [00:02<00:00, 24.83it/s]\n",
            "Processing actor Actor_20: 100%|██████████| 60/60 [00:02<00:00, 24.55it/s]\n",
            "Processing actor Actor_07: 100%|██████████| 60/60 [00:03<00:00, 15.00it/s]\n",
            "Processing actor Actor_23: 100%|██████████| 60/60 [00:03<00:00, 18.86it/s]\n",
            "Processing actor Actor_19: 100%|██████████| 60/60 [00:02<00:00, 24.72it/s]\n",
            "Processing actor Actor_22: 100%|██████████| 60/60 [00:02<00:00, 24.58it/s]\n",
            "Processing actor Actor_21: 100%|██████████| 60/60 [00:02<00:00, 24.51it/s]\n",
            "Processing actor Actor_24: 100%|██████████| 60/60 [00:03<00:00, 15.80it/s]\n",
            "Processing actor Actor_10: 100%|██████████| 60/60 [00:03<00:00, 18.96it/s]\n",
            "Processing actor Actor_02: 100%|██████████| 60/60 [00:02<00:00, 23.80it/s]\n",
            "Processing actor Actor_18: 100%|██████████| 60/60 [00:02<00:00, 23.88it/s]\n",
            "Processing actor Actor_05: 100%|██████████| 60/60 [00:02<00:00, 24.12it/s]\n"
          ]
        }
      ],
      "source": [
        "# prompt: write gradio for get audio input and use model.h5 and provode output\n",
        "\n",
        "import gradio as gr\n",
        "import librosa\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# Load the trained model and encoder\n",
        "model = keras.models.load_model('emotion_model.h5')\n",
        "# Load data (replace with your actual data loading)\n",
        "ravdess_path = \"/root/.cache/kagglehub/datasets/uwrfkaggler/ravdess-emotional-speech-audio/versions/1/audio_speech_actors_01-24/\"\n",
        "X, y, encoder = load_and_preprocess_data(ravdess_path) # Assuming you have this function defined\n",
        "\n",
        "# Feature extraction function (same as before)\n",
        "def extract_features_from_array(y, sr):\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
        "    mel = librosa.feature.melspectrogram(y=y, sr=sr)\n",
        "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "    features = np.concatenate([np.mean(mfcc, axis=1), np.mean(mel, axis=1), np.mean(chroma, axis=1)])\n",
        "    return features\n",
        "\n",
        "# Prediction function (modified to handle Gradio input)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "dgFWuA4mJ86v",
        "outputId": "150e8b09-8ab1-4689-9598-a3dd7e7d344f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://8fa985b9820605d311.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://8fa985b9820605d311.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def predict_emotion(audio_filepath): #changed from audio_file to audio_filepath\n",
        "    try:\n",
        "        # Use os.path.exists to check if the path is a valid local file\n",
        "        if os.path.exists(audio_filepath):\n",
        "            audio = AudioSegment.from_file(audio_filepath)  # No need for .name\n",
        "        else:\n",
        "            #If not a file it should be a bytesIO object (Colab or similar env)\n",
        "            audio = AudioSegment.from_file_using_temporary_files(audio_filepath)\n",
        "\n",
        "        audio = audio.set_frame_rate(22050)\n",
        "        audio_data = np.array(audio.get_array_of_samples(), dtype=np.float32)\n",
        "        audio_data = audio_data / np.max(np.abs(audio_data))  # Normalize to [-1, 1]\n",
        "\n",
        "        sr = audio.frame_rate\n",
        "\n",
        "\n",
        "        features = extract_features_from_array(audio_data, sr).reshape(1, -1, 1)\n",
        "        probabilities = model.predict(features)[0]\n",
        "        predicted_class = np.argmax(probabilities)\n",
        "        predicted_emotion = encoder.inverse_transform([predicted_class])[0]\n",
        "        return predicted_emotion\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\" # Convert exception to string for display\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=predict_emotion,\n",
        "    inputs=gr.Audio(type=\"filepath\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Emotion Recognition from Audio\",\n",
        "    description=\"Upload an audio file to predict the emotion.\"\n",
        ")\n",
        "\n",
        "\n",
        "iface.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "0AaorukMl8nV",
        "outputId": "c2c6ce25-873b-40e5-e73e-8dcef8fa67ae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a1771cff0d1a803986.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://a1771cff0d1a803986.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import librosa\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "\n",
        "# Load the trained model and encoder\n",
        "model = keras.models.load_model('emotion_model.h5')\n",
        "\n",
        "# Define emotion labels (ensure these match your trained model's labels)\n",
        "emotions = [\"angry\", \"calm\", \"disgust\", \"fearful\", \"happy\", \"neutral\", \"sad\", \"surprised\"]\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(emotions)  # Use the same order as during training\n",
        "\n",
        "emotion_colors = {\n",
        "    \"angry\": \"#FF0000\",        # Red (symbolizing anger)\n",
        "    \"calm\": \"#00CED1\",         # Dark Turquoise (symbolizing tranquility)\n",
        "    \"disgust\": \"#556B2F\",      # Dark Olive Green (symbolizing unpleasantness)\n",
        "    \"fearful\": \"#FFFF00\",      # Yellow (symbolizing caution or fear)\n",
        "    \"happy\": \"#FFD700\",        # Gold (symbolizing joy)\n",
        "    \"neutral\": \"#F5F5F5\",      # Light Gray (symbolizing neutrality)\n",
        "    \"sad\": \"#1E90FF\",          # Dodger Blue (symbolizing sadness)\n",
        "    \"surprised\": \"#FF69B4\",    # Hot Pink (symbolizing excitement or shock)\n",
        "}\n",
        "\n",
        "# Feature extraction function\n",
        "def extract_features_from_array(y, sr):\n",
        "    try:\n",
        "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
        "        mel = librosa.feature.melspectrogram(y=y, sr=sr)\n",
        "        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "        features = np.concatenate([np.mean(mfcc, axis=1), np.mean(mel, axis=1), np.mean(chroma, axis=1)])\n",
        "        return features\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Feature extraction error: {str(e)}\")\n",
        "\n",
        "# Prediction function\n",
        "def predict_emotion(audio_filepath):\n",
        "    try:\n",
        "        if os.path.exists(audio_filepath):\n",
        "            audio = AudioSegment.from_file(audio_filepath)\n",
        "        else:\n",
        "            audio = AudioSegment.from_file_using_temporary_files(audio_filepath)\n",
        "\n",
        "        # Normalize and resample audio\n",
        "        audio = audio.set_frame_rate(22050)\n",
        "        audio_data = np.array(audio.get_array_of_samples(), dtype=np.float32)\n",
        "        audio_data = audio_data / np.max(np.abs(audio_data))  # Normalize to [-1, 1]\n",
        "        sr = audio.frame_rate\n",
        "\n",
        "        # Extract features and reshape for model\n",
        "        features = extract_features_from_array(audio_data, sr).reshape(1, -1, 1)\n",
        "\n",
        "        # Make prediction\n",
        "        probabilities = model.predict(features)[0]\n",
        "        predicted_class = np.argmax(probabilities)\n",
        "        predicted_emotion = encoder.inverse_transform([predicted_class])[0]\n",
        "\n",
        "        # Get color for emotion\n",
        "        emotion_color = emotion_colors.get(predicted_emotion, \"#FFFFFF\")  # Default to white\n",
        "        return f\"<div style='width:100%; height:50px; background-color:{emotion_color}; text-align:center; color:black; line-height:50px; border-radius:10px;'>{predicted_emotion.capitalize()}</div>\"\n",
        "    except Exception as e:\n",
        "        return f\"<div style='color:red;'>Error: {str(e)}</div>\"\n",
        "\n",
        "# Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=predict_emotion,\n",
        "    inputs=gr.Audio(type=\"filepath\"),\n",
        "    outputs=gr.HTML(),  # HTML output to show styled color box\n",
        "    title=\"Emotion Recognition from Audio\",\n",
        "    description=\"Upload an audio file to predict the emotion, displayed with a corresponding color.\"\n",
        ")\n",
        "\n",
        "iface.launch()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}